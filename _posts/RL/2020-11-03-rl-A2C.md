---
layout: post
title: "强化学习之——A2C算法"
subtitle: "Advantage Actor Critic Algorithm"
author: "Roger"
header-img: "img/RL.jpg"
header-mask: 0.4
mathjax: true
tags:
  - RL
  - DL
---

## 1. 动作价值函数$Q_\pi(s_t,a_t)$
$$
\begin{align}
Q_\pi(s_t,a_t)&=\mathbb{E}_{S_{t+1},A_{t+1}}[R_t+\gamma\cdot Q_\pi(S_{t+1},A_{t+1})]\\
&=\mathbb{E}_{S_{t+1}}[R_t+\gamma\cdot\mathbb{E}_{A_{t+1}}[Q_\pi(S_{t+1},A_{t+1})]]\ \ (将A_{t+1}移到括号内)\\
&=\mathbb{E}_{S_{t+1}}[R_t+\gamma\cdot V_\pi(S_{t+1})] \tag {1} \label {eq1}
\end{align}
$$  
&emsp;&emsp;因为直接求期望不容易实现，用蒙特卡洛来近似：  
- 获得一个transition $(s_t,a_t,r_t,s_{t+1})$
- 计算无偏估计：$Q_\pi(s_t,a_t)\approx r_t+\gamma\cdot V\pi(s_{t+1})$，**该项用于Actor网络近似策略梯度**  

## 2. 状态价值函数$V_\pi(s_t)$
$$
\begin{align}
V_\pi(s_t)&=\mathbb{E}_{A_{t}}[Q_\pi(s_t,A_t)]\\
&=\mathbb{E}_{A_{t}}[\mathbb{E}_{S_{t+1}}[R_t+\gamma\cdot V_\pi(S_{t+1})]]\\
&=\mathbb{E}_{A_{t},S_{t+1}}[R_t+\gamma\cdot V_\pi(S_{t+1})] \tag {2} \label {eq2}
\end{align}
$$  
&emsp;&emsp;因为直接求期望不容易实现，用蒙特卡洛来近似：  
- 获得一个transition $(s_t,a_t,r_t,s_{t+1})$
- 计算无偏估计：$V_\pi(s_t)\approx r_t+\gamma\cdot V\pi(s_{t+1})$，**该项即为Critic网络的目标值**  

## 3. 网络的更新
### 近似策略梯度更新策略网络$\pi$（Actor）
&emsp;&emsp;随机策略梯度：$g(a_t)=\frac{\partial \rm{ln}\pi(a_t|s_t;\theta)}{\partial\theta}\cdot(Q_\pi(s_t,a_t)-V_\pi(s_t))$，用估计值代替$Q_\pi(s_t,a_t)$并用价值网络输出值$v(s_t;w)$代替$V_\pi$得到：  
$$
\begin{align}
g(a_t)&\approx\frac{\partial \rm{ln}\pi(a_t|s_t;\theta)}{\partial\theta}\cdot(r_t+\gamma\cdot v_\pi(s_{t+1};w)-v_\pi(s_t;w))\\
&=\frac{\partial \rm{ln}\pi(a_t|s_t;\theta)}{\partial\theta}\cdot(y_t-v_\pi(s_t;w)) \tag {3}\label {eq3}
\end{align}
$$  
&emsp;&emsp;得到近似策略梯度后，可用其进行梯度上升以更新策略网络$\pi$：  
$$\theta\leftarrow\theta+\beta\cdot\frac{\partial \rm{ln}\pi(a_t|s_t;\theta)}{\partial\theta}\cdot(y_t-v_\pi(s_t;w))\tag{4}\label{eq4}$$  
### TD算法更新价值网络$V$（Critic）
&emsp;&emsp;对状态价值函数$V_\pi$进行函数近似，替换为价值网络$v(s_t;w)$，得到:
$$
\begin{align}
v(s_t;w)\approx r_t+\gamma\cdot v(s_{t+1};w)\tag{5}\label{eq5}
\end{align}
$$  
&emsp;&emsp;公式$\eqref{eq5}$中右侧记为$y_t$（TD target），因为其包含了部分真实的奖励$r_t$，所以认为其更准确，可以作为目标值——>TD Learning。将价值网络预测值与TD target之间的差$\delta_t=v_\pi(s_t;w)-y_t$称为TD error。用$\frac{1}{2}\delta_t^2$作为损失函数，使用梯度下降来更新价值网络的参数：  
$$w\leftarrow w-\alpha\cdot\delta_t\cdot\frac{\partial v(s_t;w)}{\partial w}\tag{6}\label{6}$$  
&emsp;&emsp;公式$\eqref{eq3}$中$v(s_t;w)$因为$s_t$在$a_t$之前，所以该项不依赖于$a_t$。$y_t$中$s_{t+1}$与$a_t$有关，$a_t$越好，$y_t$越大，即$y_t-v(s_t;w)$为正，反之则为负。因此，Critic可以衡量当前状态下动作的好坏，$y_t-v(s_t;w)$就代表优势（Advantage）。  
## 4. A2C算法总结
1. 观察transition$(s_t,a_t,r_t,s_{t+1})$
2. 计算TD target：$y_t=r_t+\gamma\cdot v(s_{t+1};w)$
3. 计算TD error：$\delta_t=v(s_t;w)-y_t$
4. 使用近似策略梯度来更新策略网络Actor：
   $$\theta\leftarrow\theta+\beta\cdot\frac{\partial \rm{ln}\pi(a_t|s_t;\theta)}{\partial\theta}\cdot(y_t-v_\pi(s_t;w))$$
5. 使用TD error平方作为损失函数来更新价值网络Critic：
   $$w\leftarrow w-\alpha\cdot\delta_t\cdot\frac{\partial v(s_t;w)}{\partial w}$$